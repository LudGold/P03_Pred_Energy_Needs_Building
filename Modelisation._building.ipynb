{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre ici les bibliotheques dont on a besoin\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "1. Import\n",
    "2. Préparation\n",
    "3. Baseline\n",
    "4. Modèles linéaires\n",
    "5. Modèles d’arbres\n",
    "6. Méthodes d’ensemble\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Nettoyé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. CHARGEMENT ET PRÉPARATION DES DONNÉES\n",
      "Shape des données : (689, 48)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OSEBuildingID</th>\n",
       "      <th>DataYear</th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>PropertyName</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>TaxParcelIdentificationNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>DefaultData</th>\n",
       "      <th>ComplianceStatus</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>TotalGHGEmissions</th>\n",
       "      <th>GHGEmissionsIntensity</th>\n",
       "      <th>log_TotalGHGEmissions</th>\n",
       "      <th>log_SiteEnergyUse_kBtu</th>\n",
       "      <th>BuildingAge</th>\n",
       "      <th>AvgFloorArea</th>\n",
       "      <th>GFATotal_per_Building</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Mayflower park hotel</td>\n",
       "      <td>405 Olive way</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>659000030</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>249.98</td>\n",
       "      <td>2.83</td>\n",
       "      <td>5.525373</td>\n",
       "      <td>15.793246</td>\n",
       "      <td>89</td>\n",
       "      <td>7369.500000</td>\n",
       "      <td>88434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>5673-The Westin Seattle</td>\n",
       "      <td>1900 5th Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>659000475</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2089.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>7.645053</td>\n",
       "      <td>18.100297</td>\n",
       "      <td>47</td>\n",
       "      <td>23319.756098</td>\n",
       "      <td>956110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>HOTEL MAX</td>\n",
       "      <td>620 STEWART ST</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>659000640</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>286.43</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.660979</td>\n",
       "      <td>15.731637</td>\n",
       "      <td>90</td>\n",
       "      <td>6132.000000</td>\n",
       "      <td>61320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>Nonresidential COS</td>\n",
       "      <td>Other</td>\n",
       "      <td>West Precinct</td>\n",
       "      <td>810 Virginia St</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>660000560</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>301.81</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.713106</td>\n",
       "      <td>16.307609</td>\n",
       "      <td>17</td>\n",
       "      <td>48644.000000</td>\n",
       "      <td>97288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Camlin</td>\n",
       "      <td>1619 9th Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>660000825</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>176.14</td>\n",
       "      <td>2.12</td>\n",
       "      <td>5.176940</td>\n",
       "      <td>15.566239</td>\n",
       "      <td>90</td>\n",
       "      <td>7546.181818</td>\n",
       "      <td>83008.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OSEBuildingID  DataYear        BuildingType PrimaryPropertyType  \\\n",
       "0              1      2016      NonResidential               Hotel   \n",
       "1              3      2016      NonResidential               Hotel   \n",
       "2              5      2016      NonResidential               Hotel   \n",
       "3              9      2016  Nonresidential COS               Other   \n",
       "4             10      2016      NonResidential               Hotel   \n",
       "\n",
       "              PropertyName          Address     City State  ZipCode  \\\n",
       "0     Mayflower park hotel    405 Olive way  Seattle    WA  98101.0   \n",
       "1  5673-The Westin Seattle  1900 5th Avenue  Seattle    WA  98101.0   \n",
       "2                HOTEL MAX   620 STEWART ST  Seattle    WA  98101.0   \n",
       "3            West Precinct  810 Virginia St  Seattle    WA  98101.0   \n",
       "4                   Camlin  1619 9th Avenue  Seattle    WA  98101.0   \n",
       "\n",
       "   TaxParcelIdentificationNumber  ...  DefaultData ComplianceStatus  Outlier  \\\n",
       "0                      659000030  ...        False        Compliant  Unknown   \n",
       "1                      659000475  ...        False        Compliant  Unknown   \n",
       "2                      659000640  ...        False        Compliant  Unknown   \n",
       "3                      660000560  ...        False        Compliant  Unknown   \n",
       "4                      660000825  ...        False        Compliant  Unknown   \n",
       "\n",
       "   TotalGHGEmissions  GHGEmissionsIntensity  log_TotalGHGEmissions  \\\n",
       "0             249.98                   2.83               5.525373   \n",
       "1            2089.28                   2.19               7.645053   \n",
       "2             286.43                   4.67               5.660979   \n",
       "3             301.81                   3.10               5.713106   \n",
       "4             176.14                   2.12               5.176940   \n",
       "\n",
       "   log_SiteEnergyUse_kBtu  BuildingAge  AvgFloorArea  GFATotal_per_Building  \n",
       "0               15.793246           89   7369.500000                88434.0  \n",
       "1               18.100297           47  23319.756098               956110.0  \n",
       "2               15.731637           90   6132.000000                61320.0  \n",
       "3               16.307609           17  48644.000000                97288.0  \n",
       "4               15.566239           90   7546.181818                83008.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n1. CHARGEMENT ET PRÉPARATION DES DONNÉES\")\n",
    "df_work = pd.read_csv(\"clean_seattle.csv\")\n",
    "print(f\"Shape des données : {df_work.shape}\")\n",
    "df_work.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Variables numériques : 8\n",
      "✓ Variables catégorielles : 1\n",
      "✓ Variable cible : log_TotalGHGEmissions\n"
     ]
    }
   ],
   "source": [
    "num_cols = [\n",
    "    'PropertyGFATotal', 'AvgFloorArea',\n",
    "    'NumberofFloors', 'NumberofBuildings', \n",
    "    'BuildingAge', 'GFATotal_per_Building',\n",
    "    'Latitude', 'Longitude'\n",
    "]\n",
    "#  pour le moment on ne prend pas 'ENERGYSTARScore'\n",
    "cat_cols = ['PrimaryPropertyType']\n",
    "target = 'log_TotalGHGEmissions'\n",
    "print(f\"✓ Variables numériques : {len(num_cols)}\")\n",
    "print(f\"✓ Variables catégorielles : {len(cat_cols)}\")\n",
    "print(f\"✓ Variable cible : {target}\")\n",
    "\n",
    "# Préprocesseur\n",
    "preproc = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en place de la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration de la validation croisée\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Liste des métriques \n",
    "scoring = {\n",
    "    'MAE':  'neg_mean_absolute_error',\n",
    "    'RMSE': 'neg_root_mean_squared_error', \n",
    "    'R2':   'r2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je choisis ici de prendre la métrique RMSE car elle pénalise fortement les sous ou sur-estimations ce qui est crucial dans les données de type émission de carbone,\n",
      "elle est exprimée dans la même unité que la cible\n",
      "et enfin elle complète la mesure du R² en donnant une mesure concrète de l erreur moyenne attendue \n"
     ]
    }
   ],
   "source": [
    "print('Je choisis ici de prendre la métrique RMSE car elle pénalise fortement les sous ou sur-estimations ce qui est crucial dans les données de type émission de carbone,\\nelle est exprimée dans la même unité que la cible\\net enfin elle complète la mesure du R² en donnant une mesure concrète de l erreur moyenne attendue ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split réalisé : 551 train, 138 test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sélection X, y\n",
    "X = df_work[num_cols + cat_cols].copy()\n",
    "y = df_work[target]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"✓ Split réalisé : {len(X_train)} train, {len(X_test)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. MODÈLE BASELINE (DummyRegressor)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (DummyRegressor - moyenne) :\n",
      "  R² CV    : -0.0066 ± 0.0062\n",
      "  RMSE CV  : 1.4870 ± 0.1222\n",
      "  MAE CV   : 1.1643 ± 0.1213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"1. MODÈLE BASELINE (DummyRegressor)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "dummy_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', DummyRegressor(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Cross-validation du baseline\n",
    "cv_dummy = cross_validate(dummy_pipe, X_train, y_train, cv=cv, scoring=scoring)\n",
    "\n",
    "baseline_r2 = cv_dummy['test_R2'].mean()\n",
    "baseline_rmse = -cv_dummy['test_RMSE'].mean()\n",
    "baseline_mae = -cv_dummy['test_MAE'].mean()\n",
    "\n",
    "print(f\"Baseline (DummyRegressor - moyenne) :\")\n",
    "print(f\"  R² CV    : {baseline_r2:.4f} ± {cv_dummy['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE CV  : {baseline_rmse:.4f} ± {cv_dummy['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE CV   : {baseline_mae:.4f} ± {cv_dummy['test_MAE'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. RÉGRESSION LINÉAIRE SIMPLE\n",
      "--------------------------------------------------\n",
      "Régression Linéaire :\n",
      "  R² CV    : 0.2266 ± 0.2691\n",
      "  RMSE CV  : 1.2752 ± 0.1674\n",
      "  MAE CV   : 0.9305 ± 0.0767\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. RÉGRESSION LINÉAIRE SIMPLE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# La régression linéaire sert de baseline pour comprendre\n",
    "# les relations linéaires dans nos données avant d'ajouter de la régularisation\n",
    "linear_pipe = Pipeline([('prep', preproc), ('model', LinearRegression())])\n",
    "cv_linear = cross_validate(linear_pipe, X_train, y_train, cv=cv, scoring=scoring)\n",
    "\n",
    "print(f\"Régression Linéaire :\")\n",
    "print(f\"  R² CV    : {cv_linear['test_R2'].mean():.4f} ± {cv_linear['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE CV  : {-cv_linear['test_RMSE'].mean():.4f} ± {cv_linear['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE CV   : {-cv_linear['test_MAE'].mean():.4f} ± {cv_linear['test_MAE'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. MODÈLES RÉGULARISÉS (Ridge, Lasso, ElasticNet)\n",
      "--------------------------------------------------\n",
      "Optimisation des hyperparamètres par GridSearchCV...\n",
      "Meilleur modèle régularisé : ElasticNet\n",
      "Paramètres optimaux : {'model': ElasticNet(), 'model__alpha': 0.01, 'model__l1_ratio': 0.1}\n",
      "  R² CV    : 0.2655\n",
      "  RMSE CV  : 1.2515\n",
      "  MAE CV   : 0.9236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"3. MODÈLES RÉGULARISÉS (Ridge, Lasso, ElasticNet)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "#  La régularisation aide à :\n",
    "# - Ridge : réduire l'overfitting en pénalisant les coefficients élevés\n",
    "# - Lasso : sélection de variables en annulant certains coefficients\n",
    "# - ElasticNet : combinaison des avantages de Ridge et Lasso\n",
    "# modèles linéaires\n",
    "\n",
    "# Pipeline de base\n",
    "reg_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', Ridge())  # Placeholder qui sera remplacé par GridSearchCV\n",
    "])\n",
    "\n",
    "# Grille de paramètres - pour tester plusieurs combinaisons d'hyperparamètres pour donner le meilleur compromis biais/variance \n",
    "param_grid = [\n",
    "    {\n",
    "        'model': [Ridge()],\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    {\n",
    "        'model': [Lasso()],\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    {\n",
    "        'model': [ElasticNet()],\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'model__l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Optimisation des hyperparamètres par GridSearchCV...\")\n",
    "gs_reg = GridSearchCV(reg_pipe, param_grid, cv=cv, scoring=scoring, refit='RMSE', n_jobs=-1)\n",
    "gs_reg.fit(X_train, y_train)\n",
    "\n",
    "best_reg_r2 = gs_reg.cv_results_['mean_test_R2'][gs_reg.best_index_]\n",
    "best_reg_rmse = -gs_reg.best_score_\n",
    "best_reg_mae = -gs_reg.cv_results_['mean_test_MAE'][gs_reg.best_index_]\n",
    "\n",
    "print(f\"Meilleur modèle régularisé : {type(gs_reg.best_estimator_['model']).__name__}\")\n",
    "print(f\"Paramètres optimaux : {gs_reg.best_params_}\")\n",
    "print(f\"  R² CV    : {best_reg_r2:.4f}\")\n",
    "print(f\"  RMSE CV  : {best_reg_rmse:.4f}\")\n",
    "print(f\"  MAE CV   : {best_reg_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. MODÈLES ARBRES DECISIONNELS (GradientBoost et Random Forest)\n",
      "--------------------------------------------------\n",
      "→ Meilleurs params Boosting : {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 100}\n",
      "→ Meilleurs params RF : {'model__max_depth': 10, 'model__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "print(\"4. MODÈLES ARBRES DECISIONNELS (GradientBoost et Random Forest)\")\n",
    "print(\"-\"*50)\n",
    "# modèles d'arbres décisionnels - non linéaires\n",
    "# --- Pipeline “vide” pour le Gradient Boosting ---\n",
    "boost_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# --- Grille d’hyper-paramètres (préfixe 'model__') ---\n",
    "param_grid_boost = {\n",
    "    'model__n_estimators':  [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth':     [3, 5, 7]\n",
    "}\n",
    "\n",
    "# --- Lancement de la GridSearch ---\n",
    "gs_boost = GridSearchCV(\n",
    "    boost_pipe,\n",
    "    param_grid_boost,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    refit='RMSE'\n",
    ")\n",
    "gs_boost.fit(X_train, y_train)\n",
    "print(\"→ Meilleurs params Boosting :\", gs_boost.best_params_)\n",
    "\n",
    "# Pipeline “vide” pour Random Forest\n",
    "rf_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Grille de recherche (attention au préfixe model__)\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth':    [None, 5, 10]\n",
    "}\n",
    "\n",
    "# Lancement de la GridSearch\n",
    "gs_rf = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    refit='RMSE',\n",
    "    return_train_score=True\n",
    ")\n",
    "gs_rf.fit(X_train, y_train)\n",
    "print(\"→ Meilleurs params RF :\", gs_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthodes d'ensemble : Bagging / Boosting / Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. MÉTHODES D'ENSEMBLE\n",
      "--------------------------------------------------\n",
      "A) BAGGING (Bootstrap Aggregating)\n",
      "  R² CV   : 0.4194 ± 0.0290\n",
      "  RMSE CV : 1.1302 ± 0.1089\n",
      "  MAE CV  : 0.9028 ± 0.1013\n",
      "\n",
      "B) BOOSTING (Gradient Boosting)\n",
      "Paramètres optimaux : {'model__learning_rate': 0.05, 'model__max_depth': 3, 'model__max_iter': 100}\n",
      "  R² CV    : 0.3935 ± 0.0155\n",
      "  RMSE CV  : 1.1543 ± 0.0965\n",
      "  MAE CV   : 0.9277 ± 0.0907\n",
      "\n",
      "C) STACKING\n",
      "  R² CV    : 0.4125 ± 0.0877\n",
      "  RMSE CV  : 1.1320 ± 0.1093\n",
      "  MAE CV   : 0.8862 ± 0.1038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n5. MÉTHODES D'ENSEMBLE\")\n",
    "print(\"-\" * 50)\n",
    "# Techniques pour combiner plusieurs modèles\n",
    "# A) BAGGING - Réduit la variance en combinant plusieurs modèles entraînés sur\n",
    "# différents échantillons des données d'entraînement\n",
    "print(\"A) BAGGING (Bootstrap Aggregating)\")\n",
    "bagging_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', BaggingRegressor(\n",
    "        n_estimators=50,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "cv_bagging = cross_validate(bagging_pipe, X_train, y_train,cv=cv, scoring=scoring)\n",
    "print(f\"  R² CV   : {cv_bagging['test_R2'].mean():.4f} ± {cv_bagging['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE CV : {-cv_bagging['test_RMSE'].mean():.4f} ± {cv_bagging['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE CV  : {-cv_bagging['test_MAE'].mean():.4f} ± {cv_bagging['test_MAE'].std():.4f}\")\n",
    "\n",
    "# B) BOOSTING\n",
    "print(\"\\nB) BOOSTING (Gradient Boosting)\")\n",
    "# Justification : Réduit le biais en combinant séquentiellement des modèles faibles,\n",
    "# chaque nouveau modèle corrigeant les erreurs du précédent\n",
    "\n",
    "boosting_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Optimisation des hyperparamètres pour le boosting\n",
    "param_grid_boost = {\n",
    "    'model__max_iter': [100, 200],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "gs_boost = GridSearchCV(boosting_pipe, param_grid_boost, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "gs_boost.fit(X_train, y_train)\n",
    "\n",
    "cv_boost_results = cross_validate(gs_boost.best_estimator_, X_train, y_train, cv=cv, scoring=scoring)\n",
    "print(f\"Paramètres optimaux : {gs_boost.best_params_}\")\n",
    "print(f\"  R² CV    : {cv_boost_results['test_R2'].mean():.4f} ± {cv_boost_results['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE CV  : {-cv_boost_results['test_RMSE'].mean():.4f} ± {cv_boost_results['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE CV   : {-cv_boost_results['test_MAE'].mean():.4f} ± {cv_boost_results['test_MAE'].std():.4f}\")\n",
    "\n",
    "# C) STACKING\n",
    "print(\"\\nC) STACKING\")\n",
    "# Justification : Combine les prédictions de plusieurs modèles via un meta-learner\n",
    "# qui apprend comment optimiser la combinaison\n",
    "\n",
    "# Modèles de base pour le stacking\n",
    "base_models = [\n",
    "    ('ridge', gs_reg.best_estimator_['model']),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('gb', gs_boost.best_estimator_['model'])\n",
    "]\n",
    "\n",
    "stacking_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', StackingRegressor(\n",
    "        estimators=base_models,\n",
    "        final_estimator=LinearRegression(),\n",
    "        cv=3,  # CV interne pour éviter l'overfitting\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "cv_stacking = cross_validate(stacking_pipe, X_train, y_train, cv=cv, scoring=scoring)\n",
    "print(f\"  R² CV    : {cv_stacking['test_R2'].mean():.4f} ± {cv_stacking['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE CV  : {-cv_stacking['test_RMSE'].mean():.4f} ± {cv_stacking['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE CV   : {-cv_stacking['test_MAE'].mean():.4f} ± {cv_stacking['test_MAE'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. COMPARAISON DES MODÈLES (Validation Croisée)\n",
      "----------------------------------------------------------------------\n",
      "Modèle               R²       RMSE     MAE     \n",
      "--------------------------------------------------\n",
      "Baseline (Dummy)     -0.0066  1.4870   1.1643  \n",
      "Régression Linéaire  0.2266   1.2752   0.9305  \n",
      "Meilleur Régularisé  0.2655   1.2515   0.9236  \n",
      "Bagging              0.4194   1.1302   0.9028  \n",
      "Boosting             0.3935   1.1543   0.9277  \n",
      "Stacking             0.4125   1.1320   0.8862  \n",
      "\n",
      "Meilleur modèle (RMSE le plus faible) : Bagging\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. COMPARAISON DES MODÈLES (Validation Croisée)\")\n",
    "print(\"-\" * 70)\n",
    "# evaluer la meilleure performance de modèles\n",
    "results_cv = {\n",
    "    'Baseline (Dummy)': (\n",
    "        baseline_r2,\n",
    "        baseline_rmse,\n",
    "        baseline_mae\n",
    "    ),\n",
    "    'Régression Linéaire': (\n",
    "        cv_linear['test_R2'].mean(),\n",
    "        -cv_linear['test_RMSE'].mean(),\n",
    "        -cv_linear['test_MAE'].mean()\n",
    "    ),\n",
    "    'Meilleur Régularisé': (\n",
    "        best_reg_r2,\n",
    "        best_reg_rmse,\n",
    "        best_reg_mae\n",
    "    ),\n",
    "    'Bagging': (\n",
    "        cv_bagging['test_R2'].mean(),\n",
    "        -cv_bagging['test_RMSE'].mean(),\n",
    "        -cv_bagging['test_MAE'].mean()\n",
    "    ),\n",
    "    'Boosting': (\n",
    "        cv_boost_results['test_R2'].mean(),\n",
    "        -cv_boost_results['test_RMSE'].mean(),\n",
    "        -cv_boost_results['test_MAE'].mean()\n",
    "    ),\n",
    "    'Stacking': (\n",
    "        cv_stacking['test_R2'].mean(),\n",
    "        -cv_stacking['test_RMSE'].mean(),\n",
    "        -cv_stacking['test_MAE'].mean()\n",
    "    )\n",
    "}\n",
    "\n",
    "# Affichage du tableau\n",
    "print(f\"{'Modèle':<20} {'R²':<8} {'RMSE':<8} {'MAE':<8}\")\n",
    "print(\"-\" * 50)\n",
    "for model, (r2, rmse, mae) in results_cv.items():\n",
    "    print(f\"{model:<20} {r2:<8.4f} {rmse:<8.4f} {mae:<8.4f}\")\n",
    "\n",
    "# Choix du meilleur modèle sur le métrique choisi : RMSE\n",
    "best_model_name = min(results_cv.keys(), key=lambda x: results_cv[x][1])\n",
    "print(f\"\\nMeilleur modèle (RMSE le plus faible) : {best_model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation finale du meilleur modèle pour le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. ÉVALUATION FINALE SUR LE JEU DE TEST\n",
      "--------------------------------------------------\n",
      "Performance du modèle final (Bagging) sur le jeu de test :\n",
      "  R²   : 0.3432\n",
      "  RMSE : 1.1101\n",
      "  MAE  : 0.8353\n",
      "\n",
      "Amélioration vs baseline : +19.3%\n",
      "Cohérence CV/Test : 1.8% de différence\n",
      "Cohérence acceptable entre CV et test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. ÉVALUATION FINALE SUR LE JEU DE TEST\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sélection du meilleur modèle pour l'évaluation finale\n",
    "if best_model_name == 'Stacking':\n",
    "    final_model = stacking_pipe\n",
    "elif best_model_name == 'Boosting':\n",
    "    final_model = gs_boost.best_estimator_\n",
    "elif best_model_name == 'Meilleur Régularisé':\n",
    "    final_model = gs_reg.best_estimator_\n",
    "elif best_model_name == 'Bagging':\n",
    "    final_model = bagging_pipe\n",
    "else:\n",
    "    final_model = linear_pipe\n",
    "# Entraînement et prédiction sur le test\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "# Calcul des métriques finales\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Performance du modèle final ({best_model_name}) sur le jeu de test :\")\n",
    "print(f\"  R²   : {r2_test:.4f}\") #on note ici qu'avec un taux de 0.34, il y a une part importante de la variance qui n'est pas expliquée même si c'est cohérent avec notre monde\n",
    "print(f\"  RMSE : {rmse_test:.4f}\")\n",
    "print(f\"  MAE  : {mae_test:.4f}\")\n",
    "\n",
    "# Comparaison avec baseline sur test\n",
    "dummy_pipe.fit(X_train, y_train)\n",
    "y_pred_dummy_test = dummy_pipe.predict(X_test)\n",
    "rmse_dummy_test = np.sqrt(mean_squared_error(y_test, y_pred_dummy_test))\n",
    "\n",
    "improvement = (rmse_dummy_test - rmse_test) / rmse_dummy_test * 100\n",
    "print(f\"\\nAmélioration vs baseline : {improvement:+.1f}%\")\n",
    "\n",
    "# Vérification de cohérence CV vs Test\n",
    "cv_rmse = results_cv[best_model_name][1]\n",
    "coherence = abs(cv_rmse - rmse_test) / cv_rmse * 100\n",
    "print(f\"Cohérence CV/Test : {coherence:.1f}% de différence\")\n",
    "\n",
    "if coherence > 15:\n",
    "    print(\" Attention : différence importante entre CV et test\")\n",
    "else:\n",
    "    print(\"Cohérence acceptable entre CV et test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. CONCLUSION ET JUSTIFICATIONS TECHNIQUES\n",
      "----------------------------------------------------------------------\n",
      " MÉTRIQUE CHOISIE : RMSE\n",
      "  Justification : RMSE pénalise davantage les grosses erreurs, crucial pour\n",
      "  la prédiction d'émissions où les erreurs importantes sont plus problématiques.\n",
      "\n",
      " MÉTHODES TESTÉES :\n",
      "  - Baseline : établit une référence minimale\n",
      "  - Régression linéaire : comprend les relations de base\n",
      "  - Modèles régularisés : évitent l'overfitting\n",
      "  - Bagging : réduit la variance par moyennage\n",
      "  - Boosting : réduit le biais par correction séquentielle\n",
      "  - Stacking : optimise la combinaison de modèles\n",
      "\n",
      "✓ VALIDATION CROISÉE : 5-fold avec shuffle\n",
      "  Garantit une évaluation robuste et reproductible\n",
      "\n",
      "✓ MODÈLE FINAL SÉLECTIONNÉ : Bagging\n",
      "  Performance : RMSE = 1.1101\n",
      "  Amélioration vs baseline : +19.3%\n",
      " Pour aller plus loin, on pourrait tester d’autres méthodes d’ensemble comme le XGBoost ou le LightGBM\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n8. CONCLUSION ET JUSTIFICATIONS TECHNIQUES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\" MÉTRIQUE CHOISIE : RMSE\")\n",
    "print(\"  Justification : RMSE pénalise davantage les grosses erreurs, crucial pour\")\n",
    "print(\"  la prédiction d'émissions où les erreurs importantes sont plus problématiques.\")\n",
    "\n",
    "print(\"\\n MÉTHODES TESTÉES :\")\n",
    "print(\"  - Baseline : établit une référence minimale\")\n",
    "print(\"  - Régression linéaire : comprend les relations de base\")\n",
    "print(\"  - Modèles régularisés : évitent l'overfitting\")\n",
    "print(\"  - Bagging : réduit la variance par moyennage\")\n",
    "print(\"  - Boosting : réduit le biais par correction séquentielle\")\n",
    "print(\"  - Stacking : optimise la combinaison de modèles\")\n",
    "\n",
    "print(\"\\n✓ VALIDATION CROISÉE : 5-fold avec shuffle\")\n",
    "print(\"  Garantit une évaluation robuste et reproductible\")\n",
    "\n",
    "print(f\"\\n✓ MODÈLE FINAL SÉLECTIONNÉ : {best_model_name}\")\n",
    "print(f\"  Performance : RMSE = {rmse_test:.4f}\")\n",
    "print(f\"  Amélioration vs baseline : {improvement:+.1f}%\")\n",
    "print(f\" Pour aller plus loin, on pourrait tester d’autres méthodes d’ensemble comme le XGBoost ou le LightGBM\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
