{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les bibliotheques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "1. Import\n",
    "2. Préparation\n",
    "3. Baseline\n",
    "4. Modèles linéaires\n",
    "5. Modèles d’arbres\n",
    "6. Méthodes d’ensemble\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Nettoyé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. CHARGEMENT ET PRÉPARATION DES DONNÉES\n",
      "Shape des données : (689, 48)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OSEBuildingID</th>\n",
       "      <th>DataYear</th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>PropertyName</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>TaxParcelIdentificationNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>DefaultData</th>\n",
       "      <th>ComplianceStatus</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>TotalGHGEmissions</th>\n",
       "      <th>GHGEmissionsIntensity</th>\n",
       "      <th>log_TotalGHGEmissions</th>\n",
       "      <th>log_SiteEnergyUse_kBtu</th>\n",
       "      <th>BuildingAge</th>\n",
       "      <th>AvgFloorArea</th>\n",
       "      <th>GFATotal_per_Building</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Mayflower park hotel</td>\n",
       "      <td>405 Olive way</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>659000030</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>249.98</td>\n",
       "      <td>2.83</td>\n",
       "      <td>5.525373</td>\n",
       "      <td>15.793246</td>\n",
       "      <td>89</td>\n",
       "      <td>7369.500000</td>\n",
       "      <td>88434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>5673-The Westin Seattle</td>\n",
       "      <td>1900 5th Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>659000475</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2089.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>7.645053</td>\n",
       "      <td>18.100297</td>\n",
       "      <td>47</td>\n",
       "      <td>23319.756098</td>\n",
       "      <td>956110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>HOTEL MAX</td>\n",
       "      <td>620 STEWART ST</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>659000640</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>286.43</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.660979</td>\n",
       "      <td>15.731637</td>\n",
       "      <td>90</td>\n",
       "      <td>6132.000000</td>\n",
       "      <td>61320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>Nonresidential COS</td>\n",
       "      <td>Other</td>\n",
       "      <td>West Precinct</td>\n",
       "      <td>810 Virginia St</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>660000560</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>301.81</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.713106</td>\n",
       "      <td>16.307609</td>\n",
       "      <td>17</td>\n",
       "      <td>48644.000000</td>\n",
       "      <td>97288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>NonResidential</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Camlin</td>\n",
       "      <td>1619 9th Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101.0</td>\n",
       "      <td>660000825</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>176.14</td>\n",
       "      <td>2.12</td>\n",
       "      <td>5.176940</td>\n",
       "      <td>15.566239</td>\n",
       "      <td>90</td>\n",
       "      <td>7546.181818</td>\n",
       "      <td>83008.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OSEBuildingID  DataYear        BuildingType PrimaryPropertyType  \\\n",
       "0              1      2016      NonResidential               Hotel   \n",
       "1              3      2016      NonResidential               Hotel   \n",
       "2              5      2016      NonResidential               Hotel   \n",
       "3              9      2016  Nonresidential COS               Other   \n",
       "4             10      2016      NonResidential               Hotel   \n",
       "\n",
       "              PropertyName          Address     City State  ZipCode  \\\n",
       "0     Mayflower park hotel    405 Olive way  Seattle    WA  98101.0   \n",
       "1  5673-The Westin Seattle  1900 5th Avenue  Seattle    WA  98101.0   \n",
       "2                HOTEL MAX   620 STEWART ST  Seattle    WA  98101.0   \n",
       "3            West Precinct  810 Virginia St  Seattle    WA  98101.0   \n",
       "4                   Camlin  1619 9th Avenue  Seattle    WA  98101.0   \n",
       "\n",
       "   TaxParcelIdentificationNumber  ...  DefaultData ComplianceStatus  Outlier  \\\n",
       "0                      659000030  ...        False        Compliant  Unknown   \n",
       "1                      659000475  ...        False        Compliant  Unknown   \n",
       "2                      659000640  ...        False        Compliant  Unknown   \n",
       "3                      660000560  ...        False        Compliant  Unknown   \n",
       "4                      660000825  ...        False        Compliant  Unknown   \n",
       "\n",
       "   TotalGHGEmissions  GHGEmissionsIntensity  log_TotalGHGEmissions  \\\n",
       "0             249.98                   2.83               5.525373   \n",
       "1            2089.28                   2.19               7.645053   \n",
       "2             286.43                   4.67               5.660979   \n",
       "3             301.81                   3.10               5.713106   \n",
       "4             176.14                   2.12               5.176940   \n",
       "\n",
       "   log_SiteEnergyUse_kBtu  BuildingAge  AvgFloorArea  GFATotal_per_Building  \n",
       "0               15.793246           89   7369.500000                88434.0  \n",
       "1               18.100297           47  23319.756098               956110.0  \n",
       "2               15.731637           90   6132.000000                61320.0  \n",
       "3               16.307609           17  48644.000000                97288.0  \n",
       "4               15.566239           90   7546.181818                83008.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n1. CHARGEMENT ET PRÉPARATION DES DONNÉES\")\n",
    "df_work = pd.read_csv(\"clean_seattle.csv\")\n",
    "print(f\"Shape des données : {df_work.shape}\")\n",
    "df_work.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Variables numériques : 8\n",
      "✓ Variables catégorielles : 1\n",
      "✓ Variable cible : log_TotalGHGEmissions\n"
     ]
    }
   ],
   "source": [
    "num_cols = [\n",
    "    'PropertyGFATotal', 'AvgFloorArea',\n",
    "    'NumberofFloors', 'NumberofBuildings', \n",
    "    'BuildingAge', 'GFATotal_per_Building',\n",
    "    'Latitude', 'Longitude'\n",
    "]\n",
    "#  pour le moment on ne prend pas 'ENERGYSTARScore'\n",
    "cat_cols = ['PrimaryPropertyType']\n",
    "target = 'log_TotalGHGEmissions'\n",
    "print(f\"✓ Variables numériques : {len(num_cols)}\")\n",
    "print(f\"✓ Variables catégorielles : {len(cat_cols)}\")\n",
    "print(f\"✓ Variable cible : {target}\")\n",
    "\n",
    "# Préprocesseur - normaliser les données avant entrainement\n",
    "preproc = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en place de la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration de la validation croisée\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Liste des métriques \n",
    "scoring = {\n",
    "    'MAE':  'neg_mean_absolute_error',\n",
    "    'RMSE': 'neg_root_mean_squared_error', \n",
    "    'R2':   'r2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je choisis ici de prendre la métrique RMSE car elle pénalise fortement les sous ou sur-estimations ce qui est crucial dans les données de type émission de carbone,\n",
      "elle est exprimée dans la même unité que la cible\n",
      "et enfin elle complète la mesure du R² en donnant une mesure concrète de l erreur moyenne attendue \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split réalisé : 551 train, 138 test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sélection X, y - X étant les variables retenues et y la cible\n",
    "X = df_work[num_cols + cat_cols].copy()\n",
    "y = df_work[target]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"✓ Split réalisé : {len(X_train)} train, {len(X_test)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. MODÈLE BASELINE (DummyRegressor)\n",
      "--------------------------------------------------\n",
      "Baseline (DummyRegressor - moyenne) :\n",
      "  R2 CV    : -0.0066 ± 0.0062\n",
      "  RMSE CV  : 1.4870 ± 0.1222\n",
      "  MAE CV   : 1.1643 ± 0.1213\n",
      " on note ici que nos modèles à entrainer doivent avoir des résultats inférieurs à ces moyennes de base\n"
     ]
    }
   ],
   "source": [
    "# mise en place du modèle de base DummyRegressor - #prédit juste la moyenne\n",
    "print(\"1. MODÈLE BASELINE (DummyRegressor)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "dummy_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', DummyRegressor(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Cross-validation du baseline\n",
    "cv_dummy = cross_validate(dummy_pipe, X_train, y_train, cv=cv, scoring=scoring)\n",
    "\n",
    "baseline_r2 = cv_dummy['test_R2'].mean()\n",
    "baseline_rmse = -cv_dummy['test_RMSE'].mean()\n",
    "baseline_mae = -cv_dummy['test_MAE'].mean()\n",
    "\n",
    "print(f\"Baseline (DummyRegressor - moyenne) :\")\n",
    "print(f\"  R2 CV    : {baseline_r2:.4f} ± {cv_dummy['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE CV  : {baseline_rmse:.4f} ± {cv_dummy['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE CV   : {baseline_mae:.4f} ± {cv_dummy['test_MAE'].std():.4f}\")\n",
    "print(f\" on note ici que nos modèles à entrainer doivent avoir des résultats inférieurs à ces moyennes de base\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. RÉGRESSION LINÉAIRE SIMPLE\n",
      "--------------------------------------------------\n",
      "Régression Linéaire (CV interne)\n",
      "  R^2 train CV : 0.4993 ± 0.0180\n",
      "  R^2  test  CV : 0.2266 ± 0.2691\n",
      "  RMSE train CV : 1.0541 ± 0.0269\n",
      "  RMSE test  CV : 1.2752 ± 0.1674\n",
      "  MAE train CV  : 0.8251 ± 0.0232\n",
      "  MAE test  CV  : 0.9305 ± 0.0767\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. RÉGRESSION LINÉAIRE SIMPLE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# La régression linéaire sert de baseline pour comprendre\n",
    "# les relations linéaires dans nos données avant d'ajouter de la régularisation\n",
    "linear_pipe = Pipeline([('prep', preproc), ('model', LinearRegression())])\n",
    "cv_linear  = cross_validate(\n",
    "    linear_pipe,\n",
    "    X_train, y_train,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True   # ← on récupère train_X et test_X\n",
    ")\n",
    "#ici on note une petite amélioration par rapport au baseline, on explique 23% de la variance ce qui est mieux et les métriques RMSE/MAE sont plus élévés en test qu'en entrainement mais\n",
    "#de manière modérée -voir ensuite avec des modèles de regularisation (ridge/lasso) et des méthodes non linéaires pour voir une amélioration de l'écart type\n",
    "print(\"Régression Linéaire (CV interne)\")\n",
    "print(f\"  R^2 train CV : {cv_linear ['train_R2'].mean():.4f} ± {cv_linear ['train_R2'].std():.4f}\")\n",
    "print(f\"  R^2  test  CV : {cv_linear ['test_R2'].mean():.4f} ± {cv_linear ['test_R2'].std():.4f}\")\n",
    "print(f\"  RMSE train CV : {-cv_linear ['train_RMSE'].mean():.4f} ± {cv_linear ['train_RMSE'].std():.4f}\")\n",
    "print(f\"  RMSE test  CV : {-cv_linear ['test_RMSE'].mean():.4f} ± {cv_linear ['test_RMSE'].std():.4f}\")\n",
    "print(f\"  MAE train CV  : {-cv_linear ['train_MAE'].mean():.4f} ± {cv_linear ['train_MAE'].std():.4f}\")\n",
    "print(f\"  MAE test  CV  : {-cv_linear ['test_MAE'].mean():.4f} ± {cv_linear ['test_MAE'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. MODÈLES RÉGULARISÉS (Ridge, Lasso, ElasticNet)\n",
      "--------------------------------------------------\n",
      "Optimisation des hyperparamètres par GridSearchCV...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Résultats Régularisation ===\n",
      "\n",
      "Ridge :\n",
      "  Meilleurs paramètres : {'model': Ridge(), 'model__alpha': 10.0}\n",
      "  R² train CV : 0.4417\n",
      "  R² test  CV : 0.2556\n",
      "  RMSE train CV : 1.1132\n",
      "  RMSE test  CV : 1.2613\n",
      "  MAE train CV  : 0.8790\n",
      "  MAE test  CV  : 0.9347\n",
      "\n",
      "Lasso :\n",
      "  Meilleurs paramètres : {'model': Lasso(), 'model__alpha': 0.01}\n",
      "  R² train CV : 0.4485\n",
      "  R² test  CV : 0.2695\n",
      "  RMSE train CV : 1.1063\n",
      "  RMSE test  CV : 1.2531\n",
      "  MAE train CV  : 0.8705\n",
      "  MAE test  CV  : 0.9324\n",
      "\n",
      "ElasticNet :\n",
      "  Meilleurs paramètres : {'model': ElasticNet(), 'model__alpha': 0.01, 'model__l1_ratio': 0.9}\n",
      "  R² train CV : 0.4500\n",
      "  R² test  CV : 0.2673\n",
      "  RMSE train CV : 1.1048\n",
      "  RMSE test  CV : 1.2542\n",
      "  MAE train CV  : 0.8692\n",
      "  MAE test  CV  : 0.9322\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"3. MODÈLES RÉGULARISÉS (Ridge, Lasso, ElasticNet)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "#  La régularisation aide à :\n",
    "# - Ridge : réduire l'overfitting en pénalisant les coefficients élevés\n",
    "# - Lasso : sélection de variables en annulant certains coefficients\n",
    "# - ElasticNet : combinaison des avantages de Ridge et Lasso\n",
    "# modèles linéaires\n",
    "\n",
    "# Pipeline de base\n",
    "reg_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', Ridge())  # Placeholder qui sera remplacé par GridSearchCV\n",
    "])\n",
    "\n",
    "# Grille de paramètres - pour tester plusieurs combinaisons d'hyperparamètres pour donner le meilleur compromis biais/variance \n",
    "#alpha = force de pénalité pour essayer de réduire la variance (moins de sur-entrainement), stabiliser les poids de chaque feature, et sélectionner des variables via Lasso si nécessaire\n",
    "#Rideg pénalise la somme dess poids au carré, plus Alpha est grand plus les coeff sont retrécies vers 0, Lasso pénalise la somme des valeurs absolues et Elastic net combien les 2 (sélection et stabilisation)\n",
    "param_grid = [\n",
    "    {\n",
    "        'model': [Ridge()],\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0] # échelle logarithmique pour balayer plusieurs ordres de grandeur et voir si le modèle préfère une pénalité nulle, modérée ou forte\n",
    "    },\n",
    "    {\n",
    "        'model': [Lasso()],\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]  #idem\n",
    "    },\n",
    "    {\n",
    "        'model': [ElasticNet()],\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'model__l1_ratio': [0.1, 0.5, 0.7, 0.9] # curseur de mélange pour \"doser\" Lasso et Ridge - exemple 0.5 c'est 50% de Lasso et 50% de Ridge , 0.9 surtout Lasso et peu de Ridge\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Optimisation des hyperparamètres par GridSearchCV...\")\n",
    "gs_reg = GridSearchCV(\n",
    "    reg_pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    refit='RMSE', #moins standart que R^2 mais logique dans ma démarche\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True  # important pour avoir mean_train_...\n",
    ")\n",
    "gs_reg.fit(X_train, y_train) #on fit le jeu de données\n",
    "\n",
    "cv_res = gs_reg.cv_results_\n",
    "best_idx = gs_reg.best_index_                # l’indice de la meilleure config trouvée\n",
    "\n",
    "best_reg_r2   = cv_res['mean_test_R2'][best_idx]\n",
    "best_reg_rmse = -cv_res['mean_test_RMSE'][best_idx]\n",
    "best_reg_mae  = -cv_res['mean_test_MAE'][best_idx]\n",
    "\n",
    "# On récupère les meilleurs index par type\n",
    "best_ridge_idx = None\n",
    "best_lasso_idx = None\n",
    "best_enet_idx = None\n",
    "\n",
    "for i, p in enumerate(cv_res['params']):\n",
    "    if isinstance(p['model'], Ridge):\n",
    "        if best_ridge_idx is None or cv_res['mean_test_R2'][i] > cv_res['mean_test_R2'][best_ridge_idx]:\n",
    "            best_ridge_idx = i\n",
    "    elif isinstance(p['model'], Lasso):\n",
    "        if best_lasso_idx is None or cv_res['mean_test_R2'][i] > cv_res['mean_test_R2'][best_lasso_idx]:\n",
    "            best_lasso_idx = i\n",
    "    elif p['model'].__class__.__name__ == 'ElasticNet':\n",
    "        if best_enet_idx is None or cv_res['mean_test_R2'][i] > cv_res['mean_test_R2'][best_enet_idx]:\n",
    "            best_enet_idx = i\n",
    "\n",
    "print(\"\\n=== Résultats Régularisation ===\")\n",
    "\n",
    "for name, idx in [('Ridge', best_ridge_idx), ('Lasso', best_lasso_idx), ('ElasticNet', best_enet_idx)]:\n",
    "    if idx is None:\n",
    "        print(f\"{name} : Non testé\")\n",
    "        continue\n",
    "   \n",
    "    print(f\"\\n{name} :\")\n",
    "    print(f\"  Meilleurs paramètres : {cv_res['params'][idx]}\")\n",
    "    print(f\"  R² train CV : {cv_res['mean_train_R2'][idx]:.4f}\")\n",
    "    print(f\"  R² test  CV : {cv_res['mean_test_R2'][idx]:.4f}\")\n",
    "    print(f\"  RMSE train CV : {-cv_res['mean_train_RMSE'][idx]:.4f}\")\n",
    "    print(f\"  RMSE test  CV : {-cv_res['mean_test_RMSE'][idx]:.4f}\")\n",
    "    print(f\"  MAE train CV  : {-cv_res['mean_train_MAE'][idx]:.4f}\")\n",
    "    print(f\"  MAE test  CV  : {-cv_res['mean_test_MAE'][idx]:.4f}\")\n",
    "## Les trois régularisations apportent un petit gain de robustesse par rapport à la régression simple,\n",
    "# mais le R² test reste limité (~0.25), signe que d'autres modèles (Boosting, Stacking) sont nécessaires \n",
    "# pour améliorer l'entrainement.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Première exploration des modèles non-linéaires (GradientBoosting et Random Forest)\n",
      "--------------------------------------------------\n",
      "→ Meilleurs params GradientBoost : {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 100}\n",
      "→ Meilleurs params RF : {'model__max_depth': 10, 'model__n_estimators': 300}\n",
      "\n",
      "=== Résultats Gradient Boosting ===\n",
      "R² train CV : 0.7684\n",
      "R² test  CV : 0.4488\n",
      "RMSE train CV : 0.7170\n",
      "RMSE test  CV : 1.1000\n",
      "MAE train CV : 0.5687\n",
      "MAE test  CV : 0.8856\n",
      "\n",
      "=== Résultats Random Forest ===\n",
      "R² train CV : 0.8521\n",
      "R² test  CV : 0.4203\n",
      "RMSE train CV : 0.5729\n",
      "RMSE test  CV : 1.1297\n",
      "MAE train CV : 0.4670\n",
      "MAE test  CV : 0.9065\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Première exploration des modèles non-linéaires (GradientBoosting et Random Forest)\")\n",
    "print(\"-\"*50)\n",
    "# modèles d'arbres décisionnels - non linéaires - POUR VOIR SI ILS SURPASSENT LES MODELES LINEAIRES\n",
    "# --- Pipeline pour le Gradient Boosting ---\n",
    "boost_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# --- Grille d’hyper-paramètres (préfixe 'model__') ---\n",
    "param_grid_boost = {\n",
    "    'model__n_estimators':  [100, 200, 300], # nombre d'arbres déterminés, plus il y a d'arbres plus le modèle est capable de corriger les erreurs résiduelles mais plus long à entraîner\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2], # taux apprentissage - plus il est grand plus le modèle apprend vite mais risque de sur-apprentissage - 0.1 valeur par défaut - 0.01 plus lent mais plus stable et 0.2 plus rapide\n",
    "    'model__max_depth':     [3, 5, 7] # profondeur des arbres - 3 classique, 5 plus de flexibilité - 7 test si plus profond si il est risqué - AMELIORER LE COMMENTAIRE \n",
    "}\n",
    "\n",
    "# --- Lancement de la GridSearch ---\n",
    "gs_boost = GridSearchCV(\n",
    "    boost_pipe,\n",
    "    param_grid_boost,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    refit='RMSE'\n",
    ")\n",
    "gs_boost.fit(X_train, y_train)\n",
    "print(\"→ Meilleurs params GradientBoost :\", gs_boost.best_params_)\n",
    "\n",
    "# Pipeline pour Random Forest\n",
    "rf_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Grille de recherche (attention au préfixe model__)\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth':    [None, 5, 10]\n",
    "}\n",
    "\n",
    "# Lancement de la GridSearch\n",
    "gs_rf = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    refit='RMSE',\n",
    "    return_train_score=True\n",
    ")\n",
    "gs_rf.fit(X_train, y_train)\n",
    "print(\"→ Meilleurs params RF :\", gs_rf.best_params_)\n",
    "\n",
    "# Récupère tous les scores CV pour Gradient Boosting\n",
    "cv_boost = gs_boost.cv_results_\n",
    "best_boost_idx = gs_boost.best_index_\n",
    "\n",
    "print(\"\\n=== Résultats Gradient Boosting ===\")\n",
    "print(f\"R² train CV : {cv_boost['mean_train_R2'][best_boost_idx]:.4f}\")\n",
    "print(f\"R² test  CV : {cv_boost['mean_test_R2'][best_boost_idx]:.4f}\")\n",
    "print(f\"RMSE train CV : {-cv_boost['mean_train_RMSE'][best_boost_idx]:.4f}\")\n",
    "print(f\"RMSE test  CV : {-cv_boost['mean_test_RMSE'][best_boost_idx]:.4f}\")\n",
    "print(f\"MAE train CV : {-cv_boost['mean_train_MAE'][best_boost_idx]:.4f}\")\n",
    "print(f\"MAE test  CV : {-cv_boost['mean_test_MAE'][best_boost_idx]:.4f}\")\n",
    "\n",
    "# Idem pour Random Forest\n",
    "cv_rf = gs_rf.cv_results_\n",
    "best_rf_idx = gs_rf.best_index_\n",
    "\n",
    "print(\"\\n=== Résultats Random Forest ===\")\n",
    "print(f\"R² train CV : {cv_rf['mean_train_R2'][best_rf_idx]:.4f}\")\n",
    "print(f\"R² test  CV : {cv_rf['mean_test_R2'][best_rf_idx]:.4f}\")\n",
    "print(f\"RMSE train CV : {-cv_rf['mean_train_RMSE'][best_rf_idx]:.4f}\")\n",
    "print(f\"RMSE test  CV : {-cv_rf['mean_test_RMSE'][best_rf_idx]:.4f}\")\n",
    "print(f\"MAE train CV : {-cv_rf['mean_train_MAE'][best_rf_idx]:.4f}\")\n",
    "print(f\"MAE test  CV : {-cv_rf['mean_test_MAE'][best_rf_idx]:.4f}\")\n",
    "\n",
    "# interpretation des résultats\n",
    "# pour Gradient Boosting, on voit que le R2 modèle apprends bien sur le train, que le r2 test géréralise mieux que les modèles précédents (0.45 / 025 (reg lineaire-0.27 ridge/lasso/elasticNet)\n",
    "#l'écart train/test est correct pour un arbre de décision : 0.77 à 0.45 - peu de sur apprentissage - bon compromis biais/variance, RMSE test plus faible que le linéaire\n",
    "# pour Random Forest, R² train CV ≈ 0.85 → très bon fit sur le train (modèle flexible) et  R² test CV = 0.42 : très proche du Gradient Boosting, mais l’écart train/test est plus large (0.85 → 0.42).\n",
    "# Décalage indiuqant que RF est plus sur du sur-apprentissage qu'un GB réglé finement, RMSE test ≈ 1.13 : un peu plus haut que Gradient Boost, donc globalement il performe un peu moins bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthodes d'ensemble : Bagging / Boosting / Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. MÉTHODES D'ENSEMBLE\n",
      "--------------------------------------------------\n",
      "A) BAGGING (Bootstrap Aggregating)\n",
      "R^2 train : 0.7342\n",
      "R^2 test  : 0.4387\n",
      "RMSE train : 0.7684\n",
      "RMSE test  : 1.0262\n",
      "MAE train  : 0.6070\n",
      "MAE test   : 0.7663\n",
      "\n",
      "B) BOOSTING (Gradient Boosting)\n",
      "R^2 train : 0.5799\n",
      "R^2 test  : 0.3207\n",
      "RMSE train : 0.9661\n",
      "RMSE test  : 1.1289\n",
      "MAE train  : 0.7697\n",
      "MAE test   : 0.8193\n",
      "\n",
      "C) STACKING\n",
      "R^2 train : 0.7293\n",
      "R^2 test  : 0.3995\n",
      "RMSE train : 0.7754\n",
      "RMSE test  : 1.0615\n",
      "MAE train  : 0.6155\n",
      "MAE test   : 0.7871\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n5. MÉTHODES D'ENSEMBLE\")\n",
    "print(\"-\" * 50)\n",
    "# Techniques pour combiner plusieurs modèles\n",
    "# A) BAGGING - Réduit la variance en combinant plusieurs modèles entraînés sur\n",
    "# différents échantillons des données d'entraînement\n",
    "print(\"A) BAGGING (Bootstrap Aggregating)\")\n",
    "bagging_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', BaggingRegressor(\n",
    "        n_estimators=50,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "cv_bagging = cross_validate(bagging_pipe, X_train, y_train,cv=cv, scoring=scoring)\n",
    "\n",
    "best_boost = gs_boost.best_estimator_\n",
    "best_boost.fit(X_train, y_train)\n",
    "y_pred_train_boost = best_boost.predict(X_train)\n",
    "y_pred_test_boost  = best_boost.predict(X_test)\n",
    "print(f\"R^2 train : {r2_score(y_train, y_pred_train_boost):.4f}\") # gain modeste par rapport aux linéaires (≃0.25), mais écart train/test ≃0.26  \n",
    "print(f\"R^2 test  : {r2_score(y_test,  y_pred_test_boost):.4f}\") # montre un sur‐apprentissage toujours présent.  \n",
    "print(f\"RMSE train : {np.sqrt(mean_squared_error(y_train, y_pred_train_boost)):.4f}\")\n",
    "print(f\"RMSE test  : {np.sqrt(mean_squared_error(y_test,  y_pred_test_boost)):.4f}\")\n",
    "print(f\"MAE train  : {mean_absolute_error(y_train, y_pred_train_boost):.4f}\")\n",
    "print(f\"MAE test   : {mean_absolute_error(y_test,  y_pred_test_boost):.4f}\")\n",
    "\n",
    "# B) BOOSTING\n",
    "print(\"\\nB) BOOSTING (Gradient Boosting)\")\n",
    "# Justification : Réduit le biais en combinant séquentiellement des modèles faibles,\n",
    "# chaque nouveau modèle corrigeant les erreurs du précédent\n",
    "\n",
    "boosting_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Optimisation des hyperparamètres pour le boosting\n",
    "param_grid_boost = {\n",
    "    'model__max_iter': [100, 200],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "gs_boost = GridSearchCV(boosting_pipe, param_grid_boost, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "gs_boost.fit(X_train, y_train)\n",
    "\n",
    "cv_boost_results = cross_validate(gs_boost.best_estimator_, X_train, y_train, cv=cv, scoring=scoring)\n",
    "\n",
    "best_boost = gs_boost.best_estimator_\n",
    "best_boost.fit(X_train, y_train)\n",
    "y_pred_train_boost = best_boost.predict(X_train)\n",
    "y_pred_test_boost  = best_boost.predict(X_test)\n",
    "print(f\"R^2 train : {r2_score(y_train, y_pred_train_boost):.4f}\") #étonnant - même résultat que bagging \n",
    "print(f\"R^2 test  : {r2_score(y_test,  y_pred_test_boost):.4f}\")\n",
    "print(f\"RMSE train : {np.sqrt(mean_squared_error(y_train, y_pred_train_boost)):.4f}\")\n",
    "print(f\"RMSE test  : {np.sqrt(mean_squared_error(y_test,  y_pred_test_boost)):.4f}\")\n",
    "print(f\"MAE train  : {mean_absolute_error(y_train, y_pred_train_boost):.4f}\")\n",
    "print(f\"MAE test   : {mean_absolute_error(y_test,  y_pred_test_boost):.4f}\")\n",
    "\n",
    "# C) STACKING\n",
    "print(\"\\nC) STACKING\")\n",
    "# Justification : Combine les prédictions de plusieurs modèles via un meta-learner\n",
    "# qui apprend comment optimiser la combinaison\n",
    "\n",
    "# Modèles de base pour le stacking\n",
    "base_models = [\n",
    "    ('ridge', gs_reg.best_estimator_['model']),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('gb', gs_boost.best_estimator_['model'])\n",
    "]\n",
    "\n",
    "stacking_pipe = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', StackingRegressor(\n",
    "        estimators=base_models,\n",
    "        final_estimator=LinearRegression(),\n",
    "        cv=3,  # CV interne pour éviter l'overfitting\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "cv_stacking = cross_validate(stacking_pipe, X_train, y_train, cv=cv, scoring=scoring)\n",
    "stacking_pipe.fit(X_train, y_train)\n",
    "y_pred_train_stacking = stacking_pipe.predict(X_train)\n",
    "y_pred_test_stacking  = stacking_pipe.predict(X_test)\n",
    "print(f\"R^2 train : {r2_score(y_train, y_pred_train_stacking):.4f}\") # +0.08 de R2 test par rapport à Bagging/Boosting\n",
    "print(f\"R^2 test  : {r2_score(y_test,  y_pred_test_stacking):.4f}\")  #avec un écart train/test réduit (0.33 vs 0.26)   \n",
    "print(f\"RMSE train : {np.sqrt(mean_squared_error(y_train, y_pred_train_stacking)):.4f}\")# meilleure maîtrise du sur‐apprentissage.\n",
    "print(f\"RMSE test  : {np.sqrt(mean_squared_error(y_test,  y_pred_test_stacking)):.4f}\")\n",
    "print(f\"MAE train  : {mean_absolute_error(y_train, y_pred_train_stacking):.4f}\")\n",
    "print(f\"MAE test   : {mean_absolute_error(y_test,  y_pred_test_stacking):.4f}\")\n",
    "\n",
    "# Bagging est un bon compromis, a une bonne capacité à lisser la variance.\n",
    "# Boosting : un peu trop sous-appris, pourrait être mieux tuné (plus d’arbres, learning_rate ajusté).\n",
    "# Stacking : améliore Boosting mais reste un peu plus lourd et donc ne surpasse pas Bagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je choisis ici de prendre la métrique RMSE car elle pénalise fortement les sous ou sur-estimations ce qui est crucial dans les données de type émission de carbone,\n",
      "elle est exprimée dans la même unité que la cible\n",
      "et enfin elle complète la mesure du R² en donnant une mesure concrète de l erreur moyenne attendue \n"
     ]
    }
   ],
   "source": [
    "print('Je choisis ici de prendre la métrique RMSE car elle pénalise fortement les sous ou sur-estimations ce qui est crucial dans les données de type émission de carbone,\\nelle est exprimée dans la même unité que la cible\\net enfin elle complète la mesure du R² en donnant une mesure concrète de l erreur moyenne attendue ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. COMPARAISON DES MODÈLES (Validation Croisée)\n",
      "----------------------------------------------------------------------\n",
      "Modèle               R^2_CV   RMSE_CV  MAE_CV  \n",
      "--------------------------------------------------\n",
      "Baseline (Dummy)     -0.0066  1.4870   1.1643  \n",
      "Régression Linéaire  0.2266   1.2752   0.9305  \n",
      "Meilleur Régularisé  0.2655   1.2515   0.9236  \n",
      "Bagging              0.4194   1.1302   0.9028  \n",
      "Boosting             0.3935   1.1543   0.9277  \n",
      "Stacking             0.4125   1.1320   0.8862  \n",
      "\n",
      "Meilleur modèle (RMSE le plus faible) : Bagging\n",
      "\n",
      "6b. ÉVALUATION FINALE TRAIN / TEST\n",
      "----------------------------------------------------------------------\n",
      "Modèle               R^2 tr/te       RMSE tr/te      MAE tr/te      \n",
      "----------------------------------------------------------------------\n",
      "Baseline (Dummy)     0.000/-0.007     1.491/1.375      1.163/1.097      +0.0%       7.5%\n",
      "Régression Linéaire  0.487/0.392     1.067/1.068      0.833/0.817      +22.3%       16.2%\n",
      "Meilleur Régularisé  0.452/0.389     1.103/1.070      0.866/0.824      +22.2%       14.5%\n",
      "Bagging              0.917/0.343     0.429/1.110      0.338/0.835      +19.3%       1.8%\n",
      "Boosting             0.580/0.321     0.966/1.129      0.770/0.819      +17.9%       2.2%\n",
      "Stacking             0.729/0.399     0.775/1.061      0.615/0.787      +22.8%       6.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. COMPARAISON DES MODÈLES (Validation Croisée)\")\n",
    "print(\"-\" * 70)\n",
    "# evaluer la meilleure performance de modèles\n",
    "# 1) On rassemble les scores R2, RMSE, MAE calculés en cross-validation pour chaque modèle testé\n",
    "results_cv = {\n",
    "    'Baseline (Dummy)': (\n",
    "        baseline_r2,\n",
    "        baseline_rmse,\n",
    "        baseline_mae\n",
    "    ),\n",
    "    'Régression Linéaire': (\n",
    "        cv_linear ['test_R2'].mean(),\n",
    "        -cv_linear ['test_RMSE'].mean(),\n",
    "        -cv_linear ['test_MAE'].mean()\n",
    "    ),\n",
    "    \n",
    "    'Meilleur Régularisé': (\n",
    "        best_reg_r2,      \n",
    "        best_reg_rmse,\n",
    "        best_reg_mae\n",
    "    ),\n",
    "    'Bagging': (\n",
    "        cv_bagging['test_R2'].mean(),\n",
    "        -cv_bagging['test_RMSE'].mean(),\n",
    "        -cv_bagging['test_MAE'].mean()\n",
    "    ),\n",
    "    'Boosting': (\n",
    "        cv_boost_results['test_R2'].mean(),\n",
    "        -cv_boost_results['test_RMSE'].mean(),\n",
    "        -cv_boost_results['test_MAE'].mean()\n",
    "    ),\n",
    "    'Stacking': (\n",
    "        cv_stacking['test_R2'].mean(),\n",
    "        -cv_stacking['test_RMSE'].mean(),\n",
    "        -cv_stacking['test_MAE'].mean()\n",
    "    )\n",
    "}\n",
    "# Affichage formaté des résultats CV\n",
    "print(f\"{'Modèle':<20} {'R^2_CV':<8} {'RMSE_CV':<8} {'MAE_CV':<8}\")\n",
    "print(\"-\" * 50)\n",
    "for name, (r2, rmse, mae) in results_cv.items():\n",
    "    print(f\"{name:<20} {r2:<8.4f} {rmse:<8.4f} {mae:<8.4f}\")\n",
    "# Sélection du meilleur modèle en fonction du plus faible RMSE\n",
    "best_model_name = min(results_cv.keys(), key=lambda x: results_cv[x][1])\n",
    "print(f\"\\nMeilleur modèle (RMSE le plus faible) : {best_model_name}\")\n",
    "\n",
    "# 2) Évaluation finale (train vs test)\n",
    "print(\"\\n6b. ÉVALUATION FINALE TRAIN / TEST\")\n",
    "print(\"-\" * 70)\n",
    "# Ici on refait une évaluation sur train / test pour comparer : R2, RMSE et MAE sur train et test\n",
    "# On calcule aussi : l'amélioration en RMSE par rapport au Dummy sur le test\n",
    "# et La cohérence entre RMSE CV et RMSE test\n",
    "print(f\"{'Modèle':<20} {'R^2 tr/te':<15} {'RMSE tr/te':<15} {'MAE tr/te':<15}\")\n",
    "print(\"-\" * 70)\n",
    "# Map des modèles entraînés avec les meilleurs hyperparamètres\n",
    "models_map = {\n",
    "    'Baseline (Dummy)': dummy_pipe,\n",
    "    'Régression Linéaire': linear_pipe,\n",
    "    'Meilleur Régularisé': gs_reg.best_estimator_,\n",
    "    'Bagging': bagging_pipe,\n",
    "    'Boosting': gs_boost.best_estimator_,\n",
    "    'Stacking': stacking_pipe\n",
    "}\n",
    "\n",
    "for name, mdl in models_map.items():\n",
    "    # Réentraîner sur train complet\n",
    "    mdl.fit(X_train, y_train)\n",
    "    y_tr = mdl.predict(X_train)\n",
    "    y_te = mdl.predict(X_test)\n",
    "     # Scores train/test\n",
    "    r2_tr = r2_score(y_train, y_tr)\n",
    "    r2_te = r2_score(y_test,  y_te)\n",
    "    rmse_tr = np.sqrt(mean_squared_error(y_train, y_tr))\n",
    "    rmse_te = np.sqrt(mean_squared_error(y_test,  y_te))\n",
    "    mae_tr  = mean_absolute_error(y_train, y_tr)\n",
    "    mae_te  = mean_absolute_error(y_test,  y_te)\n",
    "    # amélioration vs baseline (Dummy)\n",
    "    dummy_rmse = np.sqrt(mean_squared_error(y_test, dummy_pipe.predict(X_test)))\n",
    "    improvement = (dummy_rmse - rmse_te) / dummy_rmse * 100\n",
    "    # cohérence CV/Test RMSE\n",
    "    cv_rmse = results_cv[name][1]\n",
    "    coherence = abs(cv_rmse - rmse_te) / cv_rmse * 100\n",
    "    #affichage\n",
    "    print(f\"{name:<20} {r2_tr:.3f}/{r2_te:.3f}     \"\n",
    "          f\"{rmse_tr:.3f}/{rmse_te:.3f}      \"\n",
    "          f\"{mae_tr:.3f}/{mae_te:.3f}      \"\n",
    "          f\"{improvement:+.1f}%       \"\n",
    "          f\"{coherence:.1f}%\")\n",
    "    #Gain net par rapport au Dummy valide l'intérêt du modèle.e, mais CV vs test assez inégal (16 %), signe d’un peu de variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7. CONCLUSION ET JUSTIFICATIONS TECHNIQUES\n",
    " \n",
    "\n",
    "    **Métrique choisie : RMSE**  \n",
    "    Pénalise fortement les grosses erreurs, critique pour la prédiction d’émissions de carbone où les écarts importants sont coûteux.\n",
    "    Je choisis ici de prendre la métrique RMSE car elle pénalise fortement les sous ou sur-estimations ce qui est crucial dans les données de type émission de carbone, elle est exprimée dans la même unité que la cible et enfin elle complète la mesure du R^2 en donnant une mesure concrète de l erreur moyenne attendue \n",
    "\n",
    "    **Modèle final sélectionné : BaggingRegressor**  \n",
    "        - **RMSE CV : 1.130** (meilleur de tous)  \n",
    "        - **Écart R^2 train→test : 0.92→0.34** (sur-apprentissage modéré)  \n",
    "        - **Cohérence CV/Test : 1.8 %** (le plus faible, gage de fiabilité)  \n",
    "        - **Amélioration vs baseline : +19.3 %**  \n",
    "    le Bagging a montré la meilleure cohérence entre son score de validation croisée et son score de test (1.8% d'écart). Cette stabilité me semble plus importante qu'un gain de performance minime, car elle suggère que le modèle est plus fiable et généralisera mieux à de nouvelles données\n",
    "    \n",
    "    Bagging offre donc le **meilleur compromis biais/variance** :  \n",
    "        1. **Performance** au top en validation croisée  \n",
    "        2. **Stabilité** d’une exécution à l’autre (faible variabilité)  \n",
    "        3. **Rapidité** de calcul et simplicité de tuning  \n",
    "        4. **Robustesse** aux données bruitées  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
